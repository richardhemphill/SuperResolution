{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Image Super-Resolution on Grayscale Images\n",
    "**Author:** [Richard Hemphill](mailto:rhemphill2019@my.fit.ed)<br>\n",
    "**School:** [Florida Institute of Technology](https://www.fit.edu/)<br>\n",
    "**Class:** [ECE5268 Theory of Neural Networks](http://catalog.fit.edu/preview_course_nopop.php?catoid=1&coid=927&)<br>\n",
    "**Instructor:** [Dr. Georgios C. Anagnostopoulos](https://www.fit.edu/faculty-profiles/3/georgios-anagnostopoulos/)<br>\n",
    "**Assignment:** Individual Class Project<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Concept](#Concept)\n",
    "    1. [Description](#Concept-Description)\n",
    "    1. [Goal](#Concept-Goal)\n",
    "1. [References](#References)\n",
    "    1. [Architecture](#References-Architecture)\n",
    "    1. [Dataset](#References-Dataset)\n",
    "    1. [Example](#References-Example)\n",
    "1. [Architecture](#Architecture)\n",
    "1. [Code](#Code)\n",
    "    1. [Configure](#Code-Configure)\n",
    "    1. [Utility](#Code-Utility)\n",
    "    1. [Prepocessing](#Code-Prepocessing)\n",
    "    1. [Model](#Code-Model)\n",
    "    1. [Test](#Code-Test)\n",
    "1. [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept<a class=\"anchor\" id=\"Concept\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description<a class=\"anchor\" id=\"Concept-Description\"></a>\n",
    "Using small-sized grayscale images, construct a neural network architecture that will magnify and enhance the images by a factor of 2.  The following image shows the concept where a magnified image maintains the reduces resolution and thus blurry.  A neural network can be trained to enhance features in the image (i.e. super resolution) so that it looks close to the original.\n",
    "\n",
    "![description](figures/description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal<a class=\"anchor\" id=\"Concept-Goal\"></a>\n",
    "The goal of the project is to construct Python code to import and extract a dataset, take shrunken images with the original and train a CNN (Convolutional Neural Network) to magnify and subsequently enhance an image back to the original as shown in the following figure.\n",
    "\n",
    "![goal](figures/goal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References<a name=\"References\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecute<a name=\"References-Architecute\"></a>\n",
    "The following paper is a trade study to show that a lightweigh structure can perform to a more deeper (i.e. more layers) network.  If trained sufficiently, it can outperform classical magnification (e.g. bicubic interpolation). \n",
    "> C. Dong, C. C. Loy, K. He and X. Tang, \"[Image Super-Resolution Using Deep Convolutional Networks](https://ieeexplore.ieee.org/document/7115171),\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 295-307, 1 Feb. 2016, doi: 10.1109/TPAMI.2015.2439281."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset<a name=\"References-Dataset\"></a>\n",
    "The following [Kaggle](https://www.kaggle.com/) dataset contains about 800 taining and 260 test images.  They are all greyscale and there is no license on the images.  It shows how to produce a model to perform super resolution.\n",
    "> Goose, Mr. “AlexOnly_Greyscale,” January 22, 2020. https://www.kaggle.com/spaceengineer1/alexonly-greyscale.\n",
    "\n",
    "The following walk through explains how to access Kaggle datasets from jupyter notebook.\n",
    "> Daniel, Jeff. Medium. Accessed April 11, 2021. https://medium.com/@jeff.daniel77/accessing-the-kaggle-com-api-with-jupyter-notebook-on-windows-d6f330bc6953. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example<a name=\"References-Example\"></a>\n",
    "The following example code came from the [Keras code examples](https://keras.io/examples/).  \n",
    "> Long, Xingyu. “Image Super-Resolution Using an Efficient Sub-Pixel CNN,” 2020. https://keras.io/examples/vision/super_resolution_sub_pixel/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture<a class=\"anchor\" id=\"Architecture\"></a>\n",
    "The model: creates feature maps of the low-resolution image, increases the dimensionality (i.e. number of nodes in a layer), then reconstructs the expected high-resolution image.\n",
    "\n",
    "![architecture](figures/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code<a class=\"anchor\" id=\"Code\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure<a class=\"anchor\" id=\"Code-Configure\"></a>"
   ]
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "# Path\n",
    "import os.path\n",
    "import shutil\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "\n",
    "# Image\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL.Image import BICUBIC\n",
    "import imageio\n",
    "\n",
    "# Dataset\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.test import gpu_device_name\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.image import ResizeMethod\n",
    "from tensorflow.image import psnr\n",
    "from tensorflow.nn import depth_to_space\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "source": [
    "### Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# largest integer use for seeding the random number generator\n",
    "RANDRANGE_STOP = 1000\n",
    "\n",
    "# data set parameters\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "NUM_TRAIN_PROGRESS = 100\n",
    "EPOCH_SAMPLE_PROGRESS = EPOCHS / NUM_TRAIN_PROGRESS\n",
    "\n",
    "# image parameters\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "RESCALE_FACTOR = 255.0  # normalize pixels\n",
    "CHANNELS = 1            # greyscale\n",
    "ORIG_IMG_SIZE = 64      # 64x64\n",
    "UPSCALE_FACTOR = 2      # magnification factor\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)\n",
    "\n",
    "# directoryies\n",
    "CURRENT_DIRECTORY = '.'\n",
    "FIGURE_DIRECTORY = 'figures'\n",
    "MODEL_DIRECTORY = 'variables'\n",
    "SAMPLE_DIRECTORY = 'samples'\n",
    "\n",
    "# results\n",
    "TRAINING_PLOT = 'SuperResTrain.png'\n",
    "TEST_RESULT = 'TestSetExample.png'\n",
    "SAMPLE_MOVIE = 'SampleMovie.gif'"
   ]
  },
  {
   "source": [
    "### Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure's width 8 inches, and its height 8 inches\n",
    "rcParams['figure.figsize'] = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check if Tensorflow is using GPU\n",
    "if gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TensorFlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility<a class=\"anchor\" id=\"Code-Utility\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale each pixel in the image so that intensity gets converted from 0-255 to 0.0-1.0\n",
    "def ImageNorm(img):\n",
    "    img = img/RESCALE_FACTOR\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reduced scale image of the original\n",
    "def Shrink(img, size=LOW_RES_IMG_SIZE):\n",
    "    return resize(img,[size,size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an increased scale image back to the original using bicubic interpolation.\n",
    "def Magnify(img, size=ORIG_IMG_SIZE):\n",
    "    return resize(img,[size,size],method=ResizeMethod.BICUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image that has been shrunk then magnified for enhancement.\n",
    "def PreProcess(img):\n",
    "    return Magnify(Shrink(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an increased scale image using super-resoluton model\n",
    "def EnhanceImage(model, img, size=ORIG_IMG_SIZE, channels=CHANNELS):\n",
    "    arr = img_to_array(img)\n",
    "    arr = arr.astype('float32')/RESCALE_FACTOR\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    predArr = model.predict(arr)\n",
    "    predArr *= RESCALE_FACTOR\n",
    "    predArr = predArr.reshape((size,size,channels))\n",
    "    enhancedImg = array_to_img(predArr)\n",
    "    return enhancedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    testDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.exists(testDirPre):\n",
    "        if not os.path.exists(TEST_FOLDER):\n",
    "            shutil.move(testDirPre, CURRENT_DIRECTORY)\n",
    "        \n",
    "    return topDir, TEST_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing<a class=\"anchor\" id=\"Code-Prepocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "trainFolder, testFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImgPaths = glob.glob('{}/*.{}'.format(testFolder, IMAGE_EXTENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=trainFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (PreProcess(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=trainFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (PreProcess(x),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model<a class=\"anchor\" id=\"Code-Model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        self.valLoss = []\n",
    "        self.valAcc = []\n",
    "        self.logs = []\n",
    "        self.psnr = []\n",
    "        testImg = load_img(testImgPaths[0], color_mode='grayscale', target_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE))\n",
    "        shrunkImg = testImg.resize(size=(LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE))\n",
    "        self.sampleImg = shrunkImg.resize(size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE), resample=BICUBIC)\n",
    "        self.sampleNum = 0\n",
    "        if not os.path.exists(SAMPLE_DIRECTORY):\n",
    "            os.mkdir(SAMPLE_DIRECTORY)\n",
    "\n",
    "    # Store PSNR value in each epoch.\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnrEpoch = []\n",
    "\n",
    "    # Print result of PNSR per Epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.valLoss.append(logs.get('val_loss'))\n",
    "        self.valAcc.append(logs.get('val_acc'))\n",
    "        self.psnr.append(np.mean(self.psnrEpoch))\n",
    "        # Sample images as the model is being trained\n",
    "        if (epoch % EPOCH_SAMPLE_PROGRESS) == 0:\n",
    "            sample = EnhanceImage(self.model, self.sampleImg)\n",
    "            sample.save('{}/s{:0>3}.{}'.format(SAMPLE_DIRECTORY, self.sampleNum, IMAGE_EXTENSION))\n",
    "            self.sampleNum += 1\n",
    "\n",
    "    # Aggregate PNSR per batch run\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnrEpoch.append(10 * math.log10(1 / logs[\"loss\"]))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.loss) > 1:\n",
    "            # plot the metric\n",
    "            N = np.arange(0, len(self.loss))\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(N, self.loss, label='Train Loss', color='blue')\n",
    "            ax.plot(N, self.valLoss, label='Val Loss', color='red')\n",
    "            ax.set_ylabel(\"Loss/Accuracy\")\n",
    "            ax.legend(loc='upper left')\n",
    "            ax2=ax.twinx()\n",
    "            ax2.plot(N, self.psnr, label='PSNR', color='green')\n",
    "            ax2.set_ylabel('Peak Signal to Noise Ratio')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax.set_xlabel(\"Epoch #\")\n",
    "            plt.savefig('{}/{}'.format(FIGURE_DIRECTORY, TRAINING_PLOT))\n",
    "            plt.close()\n",
    "            # animate the training\n",
    "            sampleImgs = glob.glob('{}/*.{}'.format(SAMPLE_DIRECTORY, IMAGE_EXTENSION))\n",
    "            images = []\n",
    "            for sampleImg in sampleImgs:\n",
    "                images.append(imageio.imread(sampleImg))\n",
    "            imageio.mimsave('{}/{}'.format(FIGURE_DIRECTORY, SAMPLE_MOVIE), images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "\n",
    "    conv2dArgs = {\"activation\": \"relu\",\"kernel_initializer\": \"Orthogonal\",\"padding\": \"same\"}\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=9, **conv2dArgs)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=1, **conv2dArgs)(x)\n",
    "    output = keras.layers.Conv2D(filters=1, kernel_size=5, **conv2dArgs)(x)\n",
    "\n",
    "    return keras.Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 64)    5248      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 32)    2080      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 1)     801       \n",
      "=================================================================\n",
      "Total params: 8,129\n",
      "Trainable params: 8,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load or train model\n",
    "tic = time.time()\n",
    "if os.path.exists(MODEL_DIRECTORY):\n",
    "    # to force a re-training, delete the assets and variables folder\n",
    "    sr = load_model(CURRENT_DIRECTORY)\n",
    "else:\n",
    "    sr = SuperResolution()\n",
    "    sr.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001), loss=keras.losses.MeanSquaredError())\n",
    "    sr.fit(trainSet, epochs=EPOCHS, callbacks=[SuperCallback()], validation_data=valSet, use_multiprocessing=True, verbose=0)\n",
    "    sr.save(CURRENT_DIRECTORY)\n",
    "trainTime = time.time()-tic\n",
    "sr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test<a class=\"anchor\" id=\"Code-Test\"></a>\n",
    "Test the model against a set of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image that shows the test results\n",
    "def TestImg(testImg,lowResImg,biCubicImg,superResImg):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(testImg,cmap='gray')\n",
    "    ax1.set_title('Test Image')\n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2.imshow(lowResImg,cmap='gray')\n",
    "    ax2.set_title('Low Res Image')\n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    ax3.imshow(biCubicImg,cmap='gray')\n",
    "    ax3.set_title('Bicubic Image')\n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    ax4.imshow(superResImg,cmap='gray')\n",
    "    ax4.set_title('Super Res Image')\n",
    "    plt.savefig('{}/{}'.format(FIGURE_DIRECTORY, TEST_RESULT))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the peak signal-to-noise ration of the super resolution set and compare again classical magnification.\n",
    "totalBiCubicPsnr = 0\n",
    "totalSuperPsnr = 0\n",
    "totalEnhancingTime = 0\n",
    "\n",
    "for idx, testImgPath in enumerate(testImgPaths):\n",
    "    img = load_img(testImgPath, color_mode='grayscale', target_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE))\n",
    "    lowResImg = img.resize((LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE))\n",
    "    biCubicImg = lowResImg.resize((ORIG_IMG_SIZE,ORIG_IMG_SIZE), resample=BICUBIC)\n",
    "    tic = time.time()\n",
    "    superResImg = EnhanceImage(sr,biCubicImg)\n",
    "    enhancingTime = time.time() - tic\n",
    "\n",
    "    biCubicPsnr = psnr(img_to_array(biCubicImg), img_to_array(img), max_val=255)\n",
    "    superPsnr = psnr(img_to_array(superResImg), img_to_array(img), max_val=255)\n",
    "\n",
    "    totalBiCubicPsnr += biCubicPsnr\n",
    "    totalSuperPsnr += superPsnr\n",
    "    totalEnhancingTime += enhancingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestImg(img,lowResImg,biCubicImg,superResImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results<a class=\"anchor\" id=\"Results\"></a>"
   ]
  },
  {
   "source": [
    "As the model is trained, the image quality improves.  The validation loss converges close to the training loss (i.e. no overfitting).\n",
    "![Training Plot](figures/SuperResTrain.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Images get processed over epochs to become clearer.\n",
    "\n",
    "<img src=\"figures/SampleMovie.gif\" width=\"300\" height=\"300\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![Test Set Example](figures/TestSetExample.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Magnify (BiCubic) PSNR: 25.1\nSuper Resolution PSNR: 20.4\nTraining Time (1000 epochs): 1:35:47.236868\nEnhancing Time: 37.2 ms\n"
     ]
    }
   ],
   "source": [
    "print('Magnify (BiCubic) PSNR: {:.3}'.format(totalBiCubicPsnr/(idx+1)))\n",
    "print('Super Resolution PSNR: {:.3}'.format(totalSuperPsnr/(idx+1)))\n",
    "print('Training Time ({} epochs): {:0>8}'.format(EPOCHS, str(timedelta(seconds = trainTime))))\n",
    "print('Enhancing Time: {:.3} ms'.format(1000*(totalEnhancingTime/(idx+1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}