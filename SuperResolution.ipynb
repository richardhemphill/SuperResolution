{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Image Super-Resolution on Grayscale Images\n",
    "**Author:** [Richard Hemphill](mailto:rhemphill2019@my.fit.ed)<br>\n",
    "**School:** [Florida Institute of Technology](https://www.fit.edu/)<br>\n",
    "**Class:** [ECE5268 Theory of Neural Networks](http://catalog.fit.edu/preview_course_nopop.php?catoid=1&coid=927&)<br>\n",
    "**Instructor:** [Dr. Georgios C. Anagnostopoulos](https://www.fit.edu/faculty-profiles/3/georgios-anagnostopoulos/)<br>\n",
    "**Assignment:** Individual Class Project<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Concept](#Concept)\n",
    "    1. [Description](#Concept-Description)\n",
    "    1. [Goal](#Concept-Goal)\n",
    "1. [References](#References)\n",
    "    1. [Architecture](#References-Architecture)\n",
    "    1. [Dataset](#References-Dataset)\n",
    "    1. [Example](#References-Example)\n",
    "1. [Architecture](#Architecture)\n",
    "1. [Code](#Code)\n",
    "    1. [Configure](#Code-Configure)\n",
    "    1. [Utility](#Code-Utility)\n",
    "    1. [Prepocessing](#Code-Prepocessing)\n",
    "    1. [Model](#Code-Model)\n",
    "    1. [Test](#Code-Test)\n",
    "1. [Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept<a class=\"anchor\" id=\"Concept\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description<a class=\"anchor\" id=\"Concept-Description\"></a>\n",
    "Using small-sized grayscale images, construct a neural network architecture that will magnify and enhance the images by a factor of 2.  The following image shows the concept where a magnified image maintains the reduces resolution and thus blurry.  A neural network can be trained to enhance features in the image (i.e. super resolution) so that it looks close to the original.\n",
    "\n",
    "![description](figures/description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal<a class=\"anchor\" id=\"Concept-Goal\"></a>\n",
    "The goal of the project is to construct Python code to import and extract a dataset, take shrunken images with the original and train a CNN (Convolutional Neural Network) to magnify and subsequently enhance an image back to the original as shown in the following figure.\n",
    "\n",
    "![goal](figures/goal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References<a name=\"References\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecute<a name=\"References-Architecute\"></a>\n",
    "The following paper is a trade study to show that a lightweigh structure can perform to a more deeper (i.e. more layers) network.  If trained sufficiently, it can outperform classical magnification (e.g. bicubic interpolation). \n",
    "> C. Dong, C. C. Loy, K. He and X. Tang, \"[Image Super-Resolution Using Deep Convolutional Networks](https://ieeexplore.ieee.org/document/7115171),\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 295-307, 1 Feb. 2016, doi: 10.1109/TPAMI.2015.2439281."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset<a name=\"References-Dataset\"></a>\n",
    "The following [Kaggle](https://www.kaggle.com/) dataset contains about 800 taining and 260 test images.  They are all greyscale and there is no license on the images.  It shows how to produce a model to perform super resolution.\n",
    "> Goose, Mr. “AlexOnly_Greyscale,” January 22, 2020. https://www.kaggle.com/spaceengineer1/alexonly-greyscale.\n",
    "\n",
    "The following walk through explains how to access Kaggle datasets from jupyter notebook.\n",
    "> Daniel, Jeff. Medium. Accessed April 11, 2021. https://medium.com/@jeff.daniel77/accessing-the-kaggle-com-api-with-jupyter-notebook-on-windows-d6f330bc6953. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example<a name=\"References-Example\"></a>\n",
    "The following example code came from the [Keras code examples](https://keras.io/examples/).  \n",
    "> Long, Xingyu. “Image Super-Resolution Using an Efficient Sub-Pixel CNN,” 2020. https://keras.io/examples/vision/super_resolution_sub_pixel/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture<a class=\"anchor\" id=\"Architecture\"></a>\n",
    "The model: creates feature maps of the low-resolution image, increases the dimensionality (i.e. number of nodes in a layer), then reconstructs the expected high-resolution image.\n",
    "\n",
    "![architecture](figures/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code<a class=\"anchor\" id=\"Code\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure<a class=\"anchor\" id=\"Code-Configure\"></a>"
   ]
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "# Path\n",
    "import os.path\n",
    "import shutil\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "\n",
    "# Image\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL.Image import BICUBIC\n",
    "import imageio\n",
    "\n",
    "# Dataset\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.test import gpu_device_name\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.image import ResizeMethod\n",
    "from tensorflow.image import psnr\n",
    "from tensorflow.nn import depth_to_space\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "source": [
    "### Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# largest integer use for seeding the random number generator\n",
    "RANDRANGE_STOP = 1000\n",
    "\n",
    "# data set parameters\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "NUM_TRAIN_PROGRESS = 100\n",
    "EPOCH_SAMPLE_PROGRESS = EPOCHS / NUM_TRAIN_PROGRESS\n",
    "\n",
    "# image parameters\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "RESCALE_FACTOR = 255.0  # normalize pixels\n",
    "CHANNELS = 1            # greyscale\n",
    "ORIG_IMG_SIZE = 64     # baseline size\n",
    "UPSCALE_FACTOR = 2      # magnification factor\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)\n",
    "\n",
    "# directoryies\n",
    "CURRENT_DIRECTORY = '.'\n",
    "FIGURE_DIRECTORY = 'figures'\n",
    "MODEL_DIRECTORY = 'variables'\n",
    "SAMPLE_DIRECTORY = 'samples'\n",
    "\n",
    "# results\n",
    "TRAINING_PLOT = 'SuperResTrain.png'\n",
    "TEST_RESULT = 'TestSetExample.png'\n",
    "SAMPLE_MOVIE = 'SampleMovie.gif'\n",
    "\n",
    "# print format\n",
    "FORMAT_BOLD = '\\033[1m'\n",
    "FORMAT_RED = '\\033[91m'\n",
    "FORMAT_END = '\\033[0;0m'"
   ]
  },
  {
   "source": [
    "### Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure's width 8 inches, and its height 8 inches\n",
    "rcParams['figure.figsize'] = 8, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check if Tensorflow is using GPU\n",
    "if gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TensorFlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility<a class=\"anchor\" id=\"Code-Utility\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale each pixel in the image so that intensity gets converted from 0-255 to 0.0-1.0\n",
    "def ImageNorm(img):\n",
    "    img = img/RESCALE_FACTOR\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reduced scale image of the original\n",
    "def Shrink(img, size=LOW_RES_IMG_SIZE):\n",
    "    return resize(img,[size,size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an increased scale image back to the original using bicubic interpolation.\n",
    "def Magnify(img, size=ORIG_IMG_SIZE):\n",
    "    return resize(img,[size,size],method=ResizeMethod.BICUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image that has been shrunk then magnified for enhancement.\n",
    "def PreProcess(img):\n",
    "    return Magnify(Shrink(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an increased scale image using super-resoluton model\n",
    "def EnhanceImage(model, img, size=ORIG_IMG_SIZE, channels=CHANNELS):\n",
    "    arr = img_to_array(img)\n",
    "    arr = arr.astype('float32')/RESCALE_FACTOR\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    predArr = model.predict(arr)\n",
    "    predArr *= RESCALE_FACTOR\n",
    "    predArr = predArr.reshape((size,size,channels))\n",
    "    enhancedImg = array_to_img(predArr)\n",
    "    return enhancedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    testDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.exists(testDirPre):\n",
    "        if not os.path.exists(TEST_FOLDER):\n",
    "            shutil.move(testDirPre, CURRENT_DIRECTORY)\n",
    "        \n",
    "    return topDir, TEST_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing<a class=\"anchor\" id=\"Code-Prepocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "trainFolder, testFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the test images\n",
    "testImgPaths = glob.glob('{}/*.{}'.format(testFolder, IMAGE_EXTENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a sample from the test images\n",
    "sampleIdx = random.randrange(len(testImgPaths))\n",
    "sampleFile = testImgPaths[sampleIdx]\n",
    "sampleImg = load_img(sampleFile, color_mode='grayscale', target_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE))\n",
    "sampleLowResImg = sampleImg.resize((LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE))\n",
    "sampleBiCubicImg = sampleLowResImg.resize((ORIG_IMG_SIZE,ORIG_IMG_SIZE), resample=BICUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x22C91927108>",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample Test Image: test\\1590.jpg\n"
     ]
    }
   ],
   "source": [
    "# info about sample image\n",
    "display(sampleImg)\n",
    "print('Sample Test Image: {}'.format(sampleFile))\n",
    "samplePsnr = psnr(img_to_array(sampleBiCubicImg), img_to_array(sampleImg), max_val=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x22CFB0A9CC8>",
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Interpolated PSNR: 23.9\n"
     ]
    }
   ],
   "source": [
    "# info about interpolated image\n",
    "display(sampleBiCubicImg)\n",
    "print('Interpolated PSNR: {:.3}'.format(samplePsnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=trainFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (PreProcess(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=trainFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (PreProcess(x),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model<a class=\"anchor\" id=\"Code-Model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.loss = []\n",
    "        self.valLoss = []\n",
    "        self.psnr = []\n",
    "        self.sampleImg = sampleBiCubicImg\n",
    "        self.sampleNum = 0\n",
    "        if not os.path.exists(SAMPLE_DIRECTORY):\n",
    "            os.mkdir(SAMPLE_DIRECTORY)\n",
    "\n",
    "    # Store PSNR value in each epoch.\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.psnrEpoch = []\n",
    "\n",
    "    # Print result of PNSR per Epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.valLoss.append(logs.get('val_loss'))\n",
    "        self.psnr.append(np.mean(self.psnrEpoch))\n",
    "        # Sample images as the model is being trained\n",
    "        if (epoch % EPOCH_SAMPLE_PROGRESS) == 0:\n",
    "            enhanced = EnhanceImage(self.model, self.sampleImg)\n",
    "            enhancedPsnr = psnr(img_to_array(enhanced), img_to_array(sampleImg), \n",
    "                max_val=255)\n",
    "            print('{}{}Epoch({}) PSNR({:.3}){}'.format(FORMAT_BOLD, \n",
    "                FORMAT_RED, epoch, enhancedPsnr,FORMAT_END))\n",
    "            enhanced.save('{}/s{:0>3}.{}'.format(SAMPLE_DIRECTORY, \n",
    "                self.sampleNum, IMAGE_EXTENSION))\n",
    "            self.sampleNum += 1\n",
    "\n",
    "    # Aggregate PNSR per batch run\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        self.psnrEpoch.append(10 * math.log10(1 / logs[\"loss\"]))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.loss) > 1:\n",
    "            # plot the metric\n",
    "            N = np.arange(0, len(self.loss))\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(N, self.loss, label='Train Loss', color='blue')\n",
    "            ax.plot(N, self.valLoss, label='Val Loss', color='red')\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.legend(loc='upper left')\n",
    "            ax2=ax.twinx()\n",
    "            ax2.plot(N, self.psnr, label='PSNR', color='green')\n",
    "            ax2.set_ylabel('Peak Signal to Noise Ratio')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax.set_xlabel(\"Epoch #\")\n",
    "            plt.savefig('{}/{}'.format(FIGURE_DIRECTORY, TRAINING_PLOT))\n",
    "            plt.close()\n",
    "            # animate the training\n",
    "            sampleImgs = glob.glob('{}/*.{}'.format(SAMPLE_DIRECTORY, IMAGE_EXTENSION))\n",
    "            images = []\n",
    "            for sampleImg in sampleImgs:\n",
    "                images.append(imageio.imread(sampleImg))\n",
    "            imageio.mimsave('{}/{}'.format(FIGURE_DIRECTORY, SAMPLE_MOVIE), images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "\n",
    "    conv2dArgs = {\"activation\": \"relu\",\n",
    "        \"bias_initializer\": \"zeros\",\n",
    "        \"padding\": \"same\"}\n",
    "\n",
    "    #kernInit = initializers.RandomNormal(mean=0.0, stddev=0.001)\n",
    "    kernInit = initializers.Orthogonal()\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "\n",
    "    patchExtract = keras.layers.Conv2D(filters=64, kernel_size=9,\n",
    "        kernel_initializer=kernInit,\n",
    "        **conv2dArgs)(inputs)\n",
    "\n",
    "    nonLinearMap = keras.layers.Conv2D(filters=32, kernel_size=1, \n",
    "        kernel_initializer=kernInit,\n",
    "        **conv2dArgs)(patchExtract)\n",
    "\n",
    "    reconstruction = keras.layers.Conv2D(filters=1, kernel_size=5, \n",
    "        kernel_initializer=kernInit,\n",
    "        **conv2dArgs)(nonLinearMap)\n",
    "\n",
    "    return keras.Model(inputs, reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KerasExample(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "\n",
    "    conv2dArgs = {\"activation\": \"relu\",\n",
    "        \"kernel_initializer\": \"Orthogonal\",\n",
    "        \"padding\": \"same\"}\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "\n",
    "    patchExtract1 = keras.layers.Conv2D(filters=64, kernel_size=5,\n",
    "        **conv2dArgs)(inputs)\n",
    "\n",
    "    patchExtract2 = keras.layers.Conv2D(filters=64, kernel_size=3, \n",
    "        **conv2dArgs)(patchExtract1)\n",
    "\n",
    "    patchExtract3 = keras.layers.Conv2D(filters=32, kernel_size=3, \n",
    "        **conv2dArgs)(patchExtract2)\n",
    "\n",
    "    reconstruction = keras.layers.Conv2D(filters=1, kernel_size=3, \n",
    "        **conv2dArgs)(patchExtract3)\n",
    "\n",
    "    return keras.Model(inputs, reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[91mEpoch(0) PSNR(19.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(10) PSNR(19.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(20) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(30) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(40) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(50) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(60) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(70) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(80) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(90) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(100) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(110) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(120) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(130) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(140) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(150) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(160) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(170) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(180) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(190) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(200) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(210) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(220) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(230) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(240) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(250) PSNR(19.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(260) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(270) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(280) PSNR(19.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(290) PSNR(19.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(300) PSNR(19.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(310) PSNR(19.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(320) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(330) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(340) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(350) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(360) PSNR(19.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(370) PSNR(19.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(380) PSNR(19.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(390) PSNR(19.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(400) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(410) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(420) PSNR(19.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(430) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(440) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(450) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(460) PSNR(19.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(470) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(480) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(490) PSNR(20.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(500) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(510) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(520) PSNR(20.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(530) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(540) PSNR(20.2)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(550) PSNR(20.3)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(560) PSNR(20.3)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(570) PSNR(20.4)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(580) PSNR(20.4)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(590) PSNR(20.4)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(600) PSNR(20.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(610) PSNR(20.5)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(620) PSNR(20.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(630) PSNR(20.6)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(640) PSNR(20.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(650) PSNR(20.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(660) PSNR(20.7)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(670) PSNR(20.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(680) PSNR(20.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(690) PSNR(20.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(700) PSNR(20.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(710) PSNR(20.8)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(720) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(730) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(740) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(750) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(760) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(770) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(780) PSNR(20.9)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(790) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(800) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(810) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(820) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(830) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(840) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(850) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(860) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(870) PSNR(21.0)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(880) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(890) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(900) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(910) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(920) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(930) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(940) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(950) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(960) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(970) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(980) PSNR(21.1)\u001b[0;0m\n",
      "\u001b[1m\u001b[91mEpoch(990) PSNR(21.1)\u001b[0;0m\n",
      "INFO:tensorflow:Assets written to: .\\assets\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, None, None, 64)    1664      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 32)    18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 1)     289       \n",
      "=================================================================\n",
      "Total params: 57,345\n",
      "Trainable params: 57,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load or train model\n",
    "tic = time.time()\n",
    "if os.path.exists(MODEL_DIRECTORY):\n",
    "    # to force a re-training, delete the assets and variables folder\n",
    "    sr = load_model(CURRENT_DIRECTORY)\n",
    "else:\n",
    "    #sr = SuperResolution()\n",
    "    sr = KerasExample()\n",
    "    sr.compile(optimizer=keras.optimizers.SGD(learning_rate=1E-3), \n",
    "        loss=keras.losses.MeanSquaredError())\n",
    "    sr.fit(trainSet, epochs=EPOCHS, callbacks=[SuperCallback()], \n",
    "        validation_data=valSet, use_multiprocessing=True, verbose=0)\n",
    "    sr.save(CURRENT_DIRECTORY)\n",
    "trainTime = time.time()-tic\n",
    "sr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test<a class=\"anchor\" id=\"Code-Test\"></a>\n",
    "Test the model against a set of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image that shows the test results\n",
    "def TestImg(testImg,lowResImg,biCubicImg,superResImg):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(testImg,cmap='gray')\n",
    "    ax1.set_title('Test Image')\n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2.imshow(lowResImg,cmap='gray')\n",
    "    ax2.set_title('Low Res Image')\n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    ax3.imshow(biCubicImg,cmap='gray')\n",
    "    ax3.set_title('Bicubic Image')\n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    ax4.imshow(superResImg,cmap='gray')\n",
    "    ax4.set_title('Super Res Image')\n",
    "    plt.savefig('{}/{}'.format(FIGURE_DIRECTORY, TEST_RESULT))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the peak signal-to-noise ration of the super resolution set and compare again classical magnification.\n",
    "totalBiCubicPsnr = 0\n",
    "totalSuperPsnr = 0\n",
    "totalEnhancingTime = 0\n",
    "\n",
    "for idx, testImgPath in enumerate(testImgPaths):\n",
    "    img = load_img(testImgPath, color_mode='grayscale', \n",
    "        target_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE))\n",
    "        \n",
    "    lowResImg = img.resize((LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE))\n",
    "    biCubicImg = lowResImg.resize((ORIG_IMG_SIZE,ORIG_IMG_SIZE), resample=BICUBIC)\n",
    "    tic = time.time()\n",
    "    superResImg = EnhanceImage(sr,biCubicImg)\n",
    "    enhancingTime = time.time() - tic\n",
    "\n",
    "    biCubicPsnr = psnr(img_to_array(biCubicImg), img_to_array(img), max_val=255)\n",
    "    superPsnr = psnr(img_to_array(superResImg), img_to_array(img), max_val=255)\n",
    "\n",
    "    totalBiCubicPsnr += biCubicPsnr\n",
    "    totalSuperPsnr += superPsnr\n",
    "    totalEnhancingTime += enhancingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSuperResImg = EnhanceImage(sr,sampleBiCubicImg)\n",
    "TestImg(sampleImg,sampleLowResImg,sampleBiCubicImg,sampleSuperResImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results<a class=\"anchor\" id=\"Results\"></a>"
   ]
  },
  {
   "source": [
    "As the model is trained, the image quality improves.  The validation loss converges close to the training loss (i.e. no overfitting).\n",
    "![Training Plot](figures/SuperResTrain.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Images get processed over epochs to become clearer.\n",
    "\n",
    "<img src=\"figures/SampleMovie.gif\" width=\"300\" height=\"300\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![Test Set Example](figures/TestSetExample.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Magnify (BiCubic) PSNR: 25.1\nSuper Resolution PSNR: 20.3\nTraining Time (1000 epochs): 2:18:50.237811\nEnhancing Time: 37.5 ms\n"
     ]
    }
   ],
   "source": [
    "print('Magnify (BiCubic) PSNR: {:.3}'.format(totalBiCubicPsnr/(idx+1)))\n",
    "print('Super Resolution PSNR: {:.3}'.format(totalSuperPsnr/(idx+1)))\n",
    "print('Training Time ({} epochs): {:0>8}'.format(EPOCHS, str(timedelta(seconds = trainTime))))\n",
    "print('Enhancing Time: {:.3} ms'.format(1000*(totalEnhancingTime/(idx+1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}